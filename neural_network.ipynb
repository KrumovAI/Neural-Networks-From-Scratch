{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_8J--KhrbsIX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no-WTDrXHRYu",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nHvPd8lHbdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twHYUXdMHheK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "np.set_printoptions(precision=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7REHgZXHuE9L",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKUjeypu8RGC",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLArn-oedRxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base Class\n",
        "\n",
        "class Layer:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "    \n",
        "  def forward_propagation(self, input):\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff_rxkifseDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCLayer(Layer):\n",
        "  \n",
        "  def __init__(self, output_size):    \n",
        "    self.output_size = output_size\n",
        "    self.bias = np.random.rand(1, self.output_size) - 0.5\n",
        "    \n",
        "    self.weights = None\n",
        "    \n",
        "  def forward_propagation(self, input_data):\n",
        "    if self.weights is None:\n",
        "      self.initialize(input_data)\n",
        "    \n",
        "    self.input = input_data.reshape((1, -1))\n",
        "    self.output = np.dot(self.input, self.weights) + self.bias\n",
        "    \n",
        "    return self.output\n",
        "  \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    input_error = np.dot(output_error, self.weights.T)\n",
        "    weights_error = np.dot(self.input.T, output_error)\n",
        "    \n",
        "    self.weights -= learning_rate * weights_error\n",
        "    self.bias -= learning_rate * output_error\n",
        "    \n",
        "    return input_error\n",
        "  \n",
        "  def initialize(self, input_data):        \n",
        "    self.weights = np.random.rand(input_data.size, self.output_size) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsrWSYFsdxTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActivationLayer(Layer):\n",
        "  \n",
        "  def __init__(self, activation, activation_prime):\n",
        "    self.activation = activation\n",
        "    self.activation_prime = activation_prime\n",
        "    \n",
        "  def forward_propagation(self, input_data):\n",
        "    self.input = input_data\n",
        "    self.output = self.activation(self.input)\n",
        "    \n",
        "    return self.output\n",
        "  \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return self.activation_prime(self.input, self.output) * output_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEBAWcMHCBNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DropoutLayer(Layer):\n",
        "  \n",
        "  def __init__(self, p):\n",
        "    self.p = p\n",
        "    \n",
        "  def forward_propagation(self, input_data):\n",
        "    self.dropout = np.random.binomial(1, self.p, size=input_data.shape)\n",
        "    return input_data * self.dropout\n",
        "  \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return output_error * self.dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzb-SVuGxWAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvolutionalLayerSlow(Layer):\n",
        "  \n",
        "  def __init__(self, filters_count, filter_shape, padding=(0, 0), stride=1):\n",
        "    self.filters_count = filters_count\n",
        "    self.filter_shape = filter_shape\n",
        "    self.stride = stride\n",
        "    self.padding = (padding[0], padding[1], 0)\n",
        "        \n",
        "    self.input_shape = None\n",
        "    self.input_depth = None\n",
        "\n",
        "    self.output_shape = None\n",
        "    self.weights = None\n",
        "   \n",
        "  def forward_propagation(self, input_data):\n",
        "    \n",
        "    if self.input_shape is None:\n",
        "      self.initialize(input_data)\n",
        "    \n",
        "    self.input = np.pad(input_data, ((self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]), (self.padding[2], self.padding[2])), 'constant') \n",
        "    self.output = np.zeros(self.output_shape)\n",
        "\n",
        "    for filter in range(self.filters_count):\n",
        "      for channel in range(self.input_depth):\n",
        "        row_iteration = 0\n",
        "\n",
        "        for row in range(0, self.input_shape[0], self.stride):\n",
        "          col_iteration = 0\n",
        "\n",
        "          for col in range(0, self.input_shape[1], self.stride):        \n",
        "            if row + self.filter_shape[0] >= self.input_shape[0] or col + self.filter_shape[1] >= self.input_shape[1]:\n",
        "              continue\n",
        "\n",
        "            self.output[row_iteration, col_iteration, filter] += np.sum(self.input[row : row + self.filter_shape[0], col + self.filter_shape[1], channel] * self.weights[filter])    \n",
        "\n",
        "            col_iteration += 1\n",
        "\n",
        "          row_iteration += 1\n",
        "\n",
        "    return self.output\n",
        "    \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    in_error = np.zeros(self.input_shape)\n",
        "    dWeights = np.zeros((self.filters_count, self.filter_shape[0], self.filter_shape[1]))\n",
        "    dBias = np.zeros(self.input_depth)\n",
        "\n",
        "    for filter in range(self.filters_count):\n",
        "      for channel in range(self.input_depth):\n",
        "      \n",
        "        row_iteration = 0\n",
        "\n",
        "        for row in range(output_error.shape[0]):     \n",
        "          input_row_index = row * self.stride\n",
        "\n",
        "          for col in range(output_error.shape[1]):        \n",
        "            input_col_index = col * self.stride  \n",
        "\n",
        "            if input_row_index + self.filter_shape[0] >= self.input_shape[0] or input_col_index + self.filter_shape[1] >= self.input_shape[1]:\n",
        "              continue\n",
        "\n",
        "            in_error[input_row_index : input_row_index + self.filter_shape[0], input_col_index : input_col_index + self.filter_shape[1], channel] += self.weights[filter] * output_error[row, col, filter]\n",
        "            dWeights[filter] = self.input[row_iteration : row_iteration + self.filter_shape[0], input_col_index : input_col_index + self.filter_shape[1], channel] * output_error[row, col, filter]\n",
        "\n",
        "    self.weights -= learning_rate * dWeights\n",
        "    \n",
        "    return in_error\n",
        "  \n",
        "  def initialize(self, input_data):    \n",
        "      self.input_shape = input_data.shape\n",
        "      self.input_depth = self.input_shape[2]\n",
        "    \n",
        "      self.output_shape = (int((self.input_shape[0] - self.filter_shape[0] + 2 * self.padding[0]) / self.stride + 1), int((self.input_shape[1] - self.filter_shape[1] + 2 * self.padding[1]) / self.stride + 1), self.filters_count)        \n",
        "      self.weights = np.random.rand(self.filters_count, self.filter_shape[0], self.filter_shape[1]) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dazl3Wii2HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj4kbW4wjh_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cython\n",
        "\n",
        "cimport numpy as np\n",
        "\n",
        "cpdef np.ndarray convolve_2d_stride(np.ndarray data, np.ndarray filter, np.ndarray result, int stride):\n",
        "  \n",
        "  cdef int output_rows_count = data.shape[0]\n",
        "  cdef int output_cols_count = data.shape[1]\n",
        "  \n",
        "  cdef int filter_rows_count = data.shape[0]\n",
        "  cdef int filter_cols_count = data.shape[1]\n",
        "  \n",
        "  cdef int result_rows_count = data.shape[0]\n",
        "  cdef int result_cols_count = data.shape[1]\n",
        "\n",
        "  cdef int row = 0\n",
        "  cdef int col = 0\n",
        "  \n",
        "  cdef int input_row_index = 0\n",
        "  cdef int input_col_index = 0\n",
        "    \n",
        "  for row in range(output_rows_count):     \n",
        "    input_row_index = row * stride\n",
        "\n",
        "    for col in range(output_cols_count):        \n",
        "      input_col_index = col * stride  \n",
        "\n",
        "      if input_row_index + filter_rows_count >= result_rows_count or input_col_index + filter_cols_count >= result_cols_count:\n",
        "        continue\n",
        "\n",
        "      result[input_row_index : input_row_index + result_rows_count, input_col_index : input_col_index + filter_cols_count] += filter * data[row, col]\n",
        "      \n",
        "  return result  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KBnVuL5psYlV",
        "colab": {}
      },
      "source": [
        "class ConvolutionalLayer(Layer):\n",
        "  \n",
        "  def __init__(self, filters_count, filter_shape, padding=(0, 0), stride=1):\n",
        "    self.filters_count = filters_count\n",
        "    self.filter_shape = filter_shape\n",
        "    self.stride = stride\n",
        "    self.padding = (padding[0], padding[1], 0)\n",
        "        \n",
        "    self.input_shape = None\n",
        "    self.input_depth = None\n",
        "\n",
        "    self.output_shape = None\n",
        "    self.weights = None\n",
        "    \n",
        "  def forward_propagation(self, input_data):\n",
        "    \n",
        "    if self.input_shape is None:\n",
        "      self.initialize(input_data)\n",
        "    \n",
        "    self.input = np.pad(input_data, ((self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]), (self.padding[2], self.padding[2])), 'constant') \n",
        "    self.output = np.zeros(self.output_shape)\n",
        "\n",
        "    for channel in range(self.input_depth):\n",
        "      windows = skimage.util.view_as_windows(self.input[:, :, channel], self.filter_shape, self.stride)\n",
        "      \n",
        "      for filter in range(self.filters_count):\n",
        "        self.output[:, :, filter] += np.tensordot(windows, self.weights[filter], axes=((2,3),(0,1)))\n",
        "\n",
        "    return self.output\n",
        "    \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    in_error = np.zeros(self.input_shape)\n",
        "    dWeights = np.zeros((self.filters_count, self.filter_shape[0], self.filter_shape[1]))\n",
        "\n",
        "    for channel in range(self.input_depth):\n",
        "      \n",
        "      windows = skimage.util.view_as_windows(self.input[:, :, channel], self.filter_shape, self.stride)\n",
        "      \n",
        "      for filter in range(self.filters_count):\n",
        "        in_error[:, :, channel] += convolve_2d_stride(output_error, self.weights[filter], in_error[:, :, channel], self.stride)\n",
        "        dWeights  = np.tensordot(windows, output_error[:, :, filter], axes=((0,1),(0,1)))\n",
        "  \n",
        "    self.weights -= learning_rate * dWeights\n",
        "    \n",
        "    return in_error\n",
        "  \n",
        "  def initialize(self, input_data):    \n",
        "      self.input_shape = input_data.shape\n",
        "      self.input_depth = self.input_shape[2]\n",
        "    \n",
        "      self.output_shape = (int((self.input_shape[0] - self.filter_shape[0] + 2 * self.padding[0]) / self.stride + 1), int((self.input_shape[1] - self.filter_shape[1] + 2 * self.padding[1]) / self.stride + 1), self.filters_count)        \n",
        "      self.weights = np.random.rand(self.filters_count, self.filter_shape[0], self.filter_shape[1]) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47VEAAWZDWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlattenLayer(Layer):\n",
        "  \n",
        "  def forward_propagation(self, input_data):\n",
        "    self.input_shape = input_data.shape\n",
        "    return input_data.flatten()\n",
        "  \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return output_error.reshape(self.input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1L8fjhZSlG9",
        "colab_type": "text"
      },
      "source": [
        "### Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJp2vlgYbwTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PoolingLayer(Layer):\n",
        "  \n",
        "  def __init__(self, pool_shape=(2,2), stride=2):\n",
        "    self.pool_shape = pool_shape\n",
        "    self.stride = stride\n",
        "    self.padding = (int((self.pool_shape[0] - self.stride) / 2), int((self.pool_shape[1] - self.stride) / 2), 0)\n",
        "        \n",
        "    self.input_shape = None\n",
        "    self.input_depth = None\n",
        "    \n",
        "    self.output_shape = None\n",
        "    self.weights = None\n",
        "    \n",
        "  def forward_propagation(self, input):\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def initialize(self, input_data):    \n",
        "    \n",
        "    self.input_shape = input_data.shape\n",
        "    self.input_depth = self.input_shape[2]\n",
        "    \n",
        "    self.output_shape = (int((self.input_shape[0] - self.pool_shape[0] + 2 * self.padding[0]) / self.stride + 1), int((self.input_shape[1] - self.pool_shape[1] + 2 * self.padding[1]) / self.stride + 1), self.input_depth)            \n",
        "    self.weights = np.random.rand(self.pool_shape[0], self.pool_shape[1], self.input_depth) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-HQ3PJLTsK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaxPoolingLayer(PoolingLayer):  \n",
        "  \n",
        "  def forward_propagation(self, input_data):\n",
        "    \n",
        "    if self.input_shape is None:\n",
        "      self.initialize(input_data)\n",
        "    \n",
        "    self.input = np.pad(input_data, ((self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]), (self.padding[2], self.padding[2])), 'constant')\n",
        "    self.output = np.zeros(self.output_shape)\n",
        "\n",
        "    for layer in range(self.input_depth):\n",
        "        row_iteration = 0\n",
        "\n",
        "        for row in range(0, self.input_shape[0], self.stride):\n",
        "          col_iteration = 0\n",
        "\n",
        "          for col in range(0, self.input_shape[1], self.stride):        \n",
        "            if row + self.pool_shape[0] >= self.input_shape[0] or col + self.pool_shape[1] >= self.input_shape[1]:\n",
        "              continue\n",
        "\n",
        "            self.output[row_iteration, col_iteration, layer] = np.amax(self.input[row : row + self.pool_shape[0], col : col + self.pool_shape[1], layer])\n",
        "\n",
        "            col_iteration += 1\n",
        "\n",
        "          row_iteration += 1\n",
        "\n",
        "    return self.output\n",
        "    \n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    in_error = np.zeros(self.input.shape)\n",
        "    \n",
        "    for layer in range(self.input_depth):\n",
        "\n",
        "      row_iteration = 0\n",
        "\n",
        "      for row in range(output_error.shape[0]):     \n",
        "        input_row_index = row * self.stride\n",
        "\n",
        "        for col in range(output_error.shape[1]):        \n",
        "          input_col_index = col * self.stride  \n",
        "\n",
        "          if input_row_index + self.pool_shape[0] >= self.input_shape[0] or input_col_index + self.pool_shape[1] >= self.input_shape[1]:\n",
        "            continue\n",
        "            \n",
        "          pool = self.input[input_row_index : input_row_index + self.pool_shape[0], input_col_index : input_col_index + self.pool_shape[1], layer]\n",
        "          mask = (pool == np.max(pool))\n",
        "          in_error[input_row_index : input_row_index + self.pool_shape[0], input_col_index : input_col_index + self.pool_shape[1], layer] = mask * output_error[row, col, layer]\n",
        "    \n",
        "    return in_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16RxAzwE8cyB",
        "colab_type": "text"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge-cegPE8giI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(X):\n",
        "  return np.tanh(X)\n",
        "\n",
        "def tanh_prime(X, output):\n",
        "  return 1 - np.tanh(X) ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oxNLNEqH8om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(X):\n",
        "  return np.maximum(X, np.zeros(X.shape))\n",
        "  \n",
        "def relu_prime(X, output):\n",
        "  X[X > 0] = 1\n",
        "  X[X <= 0] = 0\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEnqikoitYhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(X):\n",
        "  return 1. / (1. + np.exp(-X))\n",
        "\n",
        "def sigmoid_prime(X, output):\n",
        "  f = sigmoid(X)\n",
        "  return f * (1 - f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIo-1dQv6Omh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "  exps = np.exp(X - X.max())\n",
        "  return exps / np.sum(exps)\n",
        "\n",
        "def softmax_prime(X, output):\n",
        "  result = np.zeros(X.shape)\n",
        "\n",
        "  for i in range(len(output)):\n",
        "    for j in range(len(X)):\n",
        "      if i == j:\n",
        "        result = output[i] * (1 - X[i])\n",
        "      else: \n",
        "        result = -output[i] * X[j]\n",
        "    \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDDyiXtnTaiM",
        "colab_type": "text"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVcqr-YjTgLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(y, y_pred):    \n",
        "  return np.mean(np.power(y - y_pred, 2))\n",
        "\n",
        "def mse_prime(y, y_pred):\n",
        "  return 2 * (y_pred - y) / y.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ceS3L_lmqSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(y, y_pred):\n",
        "  epsilon = 1e-5\n",
        "  \n",
        "  return (-np.log(y_pred[y] + epsilon))\n",
        "\n",
        "def cross_entropy_prime(y, y_pred):\n",
        "  y_pred[y] -= 1  \n",
        "  \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5cN401eXwj",
        "colab_type": "text"
      },
      "source": [
        "## Main Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4CDu3vuDUbO_",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "    self.loss = None\n",
        "    self.loss_prime = None\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "    return self\n",
        "\n",
        "  def use(self, loss, loss_prime):\n",
        "    self.loss = loss\n",
        "    self.loss_prime = loss_prime\n",
        "\n",
        "    return self\n",
        "\n",
        "  def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "    samples = len(x_train)\n",
        "\n",
        "    for i in range(epochs):\n",
        "      err = 0\n",
        "\n",
        "      for j in range(samples):\n",
        "        output = x_train[j]\n",
        "        for layer in self.layers:\n",
        "          output = layer.forward_propagation(output)\n",
        "\n",
        "        err += self.loss(y_train[j], output)\n",
        "\n",
        "        error = self.loss_prime(y_train[j], output)\n",
        "        for layer in reversed(self.layers):\n",
        "            error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "      err /= samples\n",
        "      print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "    return self\n",
        "\n",
        "  def predict(self, input_data):\n",
        "      samples = len(input_data)\n",
        "      result = []\n",
        "\n",
        "      for i in range(samples):\n",
        "          output = input_data[i]\n",
        "          for layer in self.layers:\n",
        "              output = layer.forward_propagation(output)\n",
        "          result.append(output)\n",
        "\n",
        "      return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5rxKqVNXYjQ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuIJF-_3y_Lh",
        "colab_type": "text"
      },
      "source": [
        "## Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dY0UnLkH94R",
        "colab_type": "code",
        "outputId": "54007ec0-0884-474b-cd05-3e4a92b8f01b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1b0nDUAJEA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUsPj3_4DA5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUaW1VrHdYl",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfq1lUXunEr5",
        "colab_type": "code",
        "outputId": "dade1f2d-3d94-44f3-b338-40a05784f2cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "fc_net = NeuralNetwork()\n",
        "\n",
        "(fc_net\n",
        "  .add(FCLayer(100))\n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(DropoutLayer(p=0.75))\n",
        "  .add(FCLayer(50))\n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(FCLayer(10))\n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(ActivationLayer(softmax, softmax_prime))\n",
        "  .use(mse, mse_prime)\n",
        "  .fit(x_train[0:1000], y_train[0:1000], epochs=30, learning_rate=0.1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/30   error=0.093278\n",
            "epoch 2/30   error=0.089956\n",
            "epoch 3/30   error=0.088449\n",
            "epoch 4/30   error=0.087800\n",
            "epoch 5/30   error=0.086694\n",
            "epoch 6/30   error=0.085783\n",
            "epoch 7/30   error=0.084316\n",
            "epoch 8/30   error=0.082568\n",
            "epoch 9/30   error=0.080677\n",
            "epoch 10/30   error=0.077986\n",
            "epoch 11/30   error=0.075121\n",
            "epoch 12/30   error=0.071991\n",
            "epoch 13/30   error=0.069615\n",
            "epoch 14/30   error=0.066807\n",
            "epoch 15/30   error=0.065202\n",
            "epoch 16/30   error=0.063865\n",
            "epoch 17/30   error=0.061491\n",
            "epoch 18/30   error=0.061052\n",
            "epoch 19/30   error=0.059308\n",
            "epoch 20/30   error=0.058032\n",
            "epoch 21/30   error=0.057582\n",
            "epoch 22/30   error=0.056205\n",
            "epoch 23/30   error=0.055539\n",
            "epoch 24/30   error=0.055000\n",
            "epoch 25/30   error=0.054030\n",
            "epoch 26/30   error=0.053283\n",
            "epoch 27/30   error=0.052523\n",
            "epoch 28/30   error=0.051948\n",
            "epoch 29/30   error=0.051270\n",
            "epoch 30/30   error=0.051382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NeuralNetwork at 0x7feea8c8b898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXj9vf0B0ifM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = net.predict(x_test[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w96zqtEH6Z-",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJMQWnzuIjvX",
        "colab_type": "code",
        "outputId": "8beb6e13-88f8-4282-c942-ce78ea87ade4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "conv_net = NeuralNetwork()\n",
        "\n",
        "(conv_net\n",
        "  .add(ConvolutionalLayer(filter_shape=(3, 3), filters_count=5, stride=2, padding=(1,1)))\n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(MaxPoolingLayer(pool_shape=(2, 2)))\n",
        "  .add(FlattenLayer())\n",
        "  .add(FCLayer(100))\n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(DropoutLayer(p=0.75))\n",
        "  .add(FCLayer(50))  \n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(FCLayer(10))\n",
        "  .add(FlattenLayer())\n",
        "  .add(ActivationLayer(tanh, tanh_prime))\n",
        "  .add(ActivationLayer(softmax, softmax_prime))\n",
        "  .use(mse, mse_prime)\n",
        "  .fit(x_train[0:1000], y_train[0:1000], epochs=30, learning_rate=0.1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/30   error=0.090917\n",
            "epoch 2/30   error=0.085968\n",
            "epoch 3/30   error=0.080977\n",
            "epoch 4/30   error=0.076153\n",
            "epoch 5/30   error=0.072818\n",
            "epoch 6/30   error=0.070183\n",
            "epoch 7/30   error=0.067707\n",
            "epoch 8/30   error=0.065412\n",
            "epoch 9/30   error=0.063300\n",
            "epoch 10/30   error=0.062564\n",
            "epoch 11/30   error=0.060378\n",
            "epoch 12/30   error=0.059850\n",
            "epoch 13/30   error=0.058600\n",
            "epoch 14/30   error=0.058431\n",
            "epoch 15/30   error=0.056987\n",
            "epoch 16/30   error=0.055938\n",
            "epoch 17/30   error=0.055670\n",
            "epoch 18/30   error=0.054547\n",
            "epoch 19/30   error=0.054616\n",
            "epoch 20/30   error=0.054824\n",
            "epoch 21/30   error=0.053635\n",
            "epoch 22/30   error=0.053197\n",
            "epoch 23/30   error=0.053238\n",
            "epoch 24/30   error=0.053151\n",
            "epoch 25/30   error=0.052795\n",
            "epoch 26/30   error=0.052544\n",
            "epoch 27/30   error=0.052048\n",
            "epoch 28/30   error=0.051052\n",
            "epoch 29/30   error=0.052164\n",
            "epoch 30/30   error=0.051023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NeuralNetwork at 0x7feea8c79fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iCipcT2o2yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_preds = conv_net.predict(x_test[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcQDGJ_6jp48",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GGuDEGpjuyr",
        "colab_type": "text"
      },
      "source": [
        "Keep in mind that we've only run a small portion of the data and the result will not be too astonishing. But you know why this implementation is badass. It's cause you've gained knowledge!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k8aTSUDIk3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUlBxREhUdeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normal_results = [np.argmax(sample) for sample in preds]\n",
        "conv_results = [np.argmax(sample) for sample in conv_preds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90LGsQkQgAua",
        "colab_type": "code",
        "outputId": "73e83d43-3060-4286-925f-6e690bd6fe83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy | Normal net: {}; Conv Net: {}'.format(accuracy_score(y_test[:100], normal_results), accuracy_score(y_test[:100], conv_results)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy | Normal net: 0.7; Conv Net: 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8J--KhrbsIX",
        "colab_type": "text"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEC4ISdbvhc",
        "colab_type": "code",
        "outputId": "184010af-ddfe-4955-b60d-674dd6583948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0711 13:31:44.495379 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0711 13:31:44.525401 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0711 13:31:44.535834 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0711 13:31:44.574197 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0711 13:31:44.577066 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0711 13:31:44.588515 139740251035520 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0711 13:31:44.660979 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0711 13:31:44.670426 139740251035520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0711 13:31:44.780761 139740251035520 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.2522 - acc: 0.9205 - val_loss: 0.0556 - val_acc: 0.9820\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0884 - acc: 0.9742 - val_loss: 0.0403 - val_acc: 0.9868\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0641 - acc: 0.9811 - val_loss: 0.0348 - val_acc: 0.9886\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0531 - acc: 0.9847 - val_loss: 0.0317 - val_acc: 0.9894\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0460 - acc: 0.9855 - val_loss: 0.0324 - val_acc: 0.9886\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0403 - acc: 0.9876 - val_loss: 0.0331 - val_acc: 0.9892\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0374 - acc: 0.9889 - val_loss: 0.0285 - val_acc: 0.9906\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0342 - acc: 0.9891 - val_loss: 0.0257 - val_acc: 0.9918\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0310 - acc: 0.9905 - val_loss: 0.0274 - val_acc: 0.9909\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0287 - acc: 0.9911 - val_loss: 0.0261 - val_acc: 0.9914\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0265 - acc: 0.9918 - val_loss: 0.0265 - val_acc: 0.9917\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.0251 - val_acc: 0.9926\n",
            "Test loss: 0.0251252648020788\n",
            "Test accuracy: 0.9926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unSvbTm5bxqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}